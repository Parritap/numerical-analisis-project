
\section{Objetivo}
Desarrollar e implementar funciones en MATLAB que apliquen los métodos numéricos vistos en clase para resolver problemas de ingeniería, enseñados en la asignatura Análisis numérico y que nos proporciona el docente Edwin Romero Cuero.

\noindent
Buscamos a través de esta actividad, fortalecer la comprensión de los distintos métodos de análisis aprendidos durante este semestre, a su vez comprender los errores de truncamiento, técnicas de búsqueda de raíces, regresiones, interpolación y ecuaciones diferenciales.

% Contenido principal del documento
\section{Introducción}

En esta actividad se desea utilizar varios métodos numéricos y aplicarlos en un sistema de cómputo numérico ``MATLAB'', buscando darle una solución a problemas que son comúnmente vistos en el ámbito de la ingeniería, donde se tratan temas como series de Taylor, métodos de búsqueda de raíces, regresión, interpolación y el uso de los métodos de Euler y Runge-Kutta para resoluciones numéricas de ecuaciones diferenciales. Mediante el uso de los conocimientos apropiados en el ambiente de desarrollo se planea poner en pruebas los conocimientos teóricos con problemas recurrentes en el área laboral.

Los métodos numéricos constituyen técnicas mediante las cuales es posible formular problemas matemáticos de tal forma que puedan resolverse utilizando operaciones aritméticas. Aunque existen muchos tipos de métodos numéricos, todos comparten una característica común: invariablemente se debe realizar un buen número de cálculos aritméticos. No es raro que con el desarrollo de computadoras digitales eficientes y rápidas, el papel de los métodos numéricos en la solución de problemas de ingeniería haya aumentado considerablemente en los últimos años.

\newpage

\section{Métodos para encontrar raíces}

La búsqueda de raíces es uno de los problemas fundamentales en análisis numérico. Según \cite{Delic2026} la raíz de una función de una variable es el valor del argumento para el cual la función tiene un valor de cero. Estos valores también se conocen como ceros de la función. La importancia de encontrar raíces radica en que muchos problemas de ingeniería pueden reducirse a la solución de ecuaciones de la forma $f(x) = 0$.

\subsection{Método de Bisección}

El método de bisección, también llamado dicotomía, es un algoritmo de búsqueda de raíces que trabaja dividiendo el intervalo a la mitad y seleccionando el subintervalo que tiene la raíz.

\subsubsection{Fundamento teórico}

El método de bisección se basa en el Teorema del Valor Intermedio, el cual establece que si una función continua $f(x)$ toma valores de signos opuestos en los extremos de un intervalo $[a, b]$, es decir, si $f(a) \cdot f(b) < 0$, entonces existe al menos una raíz en ese intervalo.

\subsubsection{Algoritmo}

El procedimiento del método de bisección es el siguiente:

\begin{enumerate}
	\item Se elige un intervalo inicial $[a, b]$ tal que $f(a) \cdot f(b) < 0$
	\item Se calcula el punto medio: $c = \frac{a + b}{2}$
	\item Se evalúa $f(c)$:
	      \begin{itemize}
		      \item Si $f(c) = 0$, entonces $c$ es la raíz exacta
		      \item Si $f(a) \cdot f(c) < 0$, la raíz está en $[a, c]$, por lo que $b = c$
		      \item Si $f(c) \cdot f(b) < 0$, la raíz está en $[c, b]$, por lo que $a = c$
	      \end{itemize}
	\item Se repite el proceso hasta que $|b - a| < \epsilon$ o $|f(c)| < \epsilon$, donde $\epsilon$ es la tolerancia deseada
\end{enumerate}

\subsubsection{Ventajas y desventajas}

\textbf{Ventajas:}
\begin{itemize}
	\item Siempre converge si se cumple la condición inicial
	\item Es simple de implementar
	\item Garantiza encontrar una raíz si existe en el intervalo
\end{itemize}

\textbf{Desventajas:}
\begin{itemize}
	\item Convergencia lenta (lineal)
	\item Requiere conocer un intervalo que contenga la raíz
	\item No puede encontrar raíces múltiples fácilmente
\end{itemize}

\subsubsection{Error y convergencia}

El error absoluto después de $n$ iteraciones está acotado por:
\begin{equation}
	|e_n| \leq \frac{b - a}{2^n}
\end{equation}

donde $b - a$ es la longitud del intervalo inicial.

\subsection{Método de Newton-Raphson}

El método de Newton-Raphson es un algoritmo iterativo para encontrar raíces de funciones reales. Utiliza la derivada de la función para aproximar la raíz a partir de un valor inicial. Es uno de los métodos más potentes y ampliamente utilizados para resolver ecuaciones no lineales.

\subsubsection{Fundamento teórico}

El método se basa en la aproximación lineal de la función mediante su serie de Taylor. Si $x_n$ es una aproximación a la raíz, entonces la siguiente aproximación $x_{n+1}$ se obtiene encontrando donde la línea tangente a la curva en el punto $(x_n, f(x_n))$ cruza el eje $x$.

\subsubsection{Fórmula iterativa}

La fórmula de Newton-Raphson es:
\begin{equation}
	x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
\end{equation}

Geométricamente, esto representa extender la tangente a la curva en el punto $(x_n, f(x_n))$ hasta que intersecte el eje $x$.

\subsubsection{Algoritmo}

\begin{enumerate}
	\item Elegir un valor inicial $x_0$ cercano a la raíz esperada
	\item Calcular $f(x_n)$ y $f'(x_n)$
	\item Verificar que $f'(x_n) \neq 0$
	\item Calcular la siguiente aproximación: $x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}$
	\item Verificar el criterio de convergencia: $|x_{n+1} - x_n| < \epsilon$ o $|f(x_{n+1})| < \epsilon$
	\item Si no se cumple el criterio, repetir desde el paso 2
\end{enumerate}

\subsubsection{Ventajas y desventajas}

\textbf{Ventajas:}
\begin{itemize}
	\item Convergencia cuadrática cuando se está cerca de la raíz
	\item Requiere menos iteraciones que bisección
	\item Muy preciso cuando converge
\end{itemize}

\textbf{Desventajas:}
\begin{itemize}
	\item Requiere calcular la derivada $f'(x)$
	\item No garantiza convergencia si la elección inicial es mala
	\item Puede diverger o ciclar si $f'(x)$ es muy pequeña
	\item Sensible a la elección del punto inicial
\end{itemize}

\subsubsection{Condiciones de convergencia}

El método converge si:
\begin{itemize}
	\item La función es continuamente diferenciable
	\item La derivada no se anula en la vecindad de la raíz
	\item El valor inicial $x_0$ está suficientemente cerca de la raíz
\end{itemize}

\subsection{Método de la Secante}

El método de la secante es una variación del método de Newton-Raphson que no requiere calcular la derivada. En su lugar, aproxima la derivada usando dos puntos anteriores:

\begin{equation}
	x_{n+1} = x_n - f(x_n) \frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}
\end{equation}

Este método tiene convergencia superlineal (orden aproximadamente 1.618) y es útil cuando el cálculo de la derivada es costoso o difícil.

\section{Ajuste de curvas}

El ajuste de curvas es una técnica fundamental en análisis numérico que busca encontrar una función que mejor represente un conjunto de datos experimentales o medidos. Este proceso es esencial en ingeniería para modelar fenómenos físicos, predecir comportamientos y analizar tendencias.

\subsection{Regresión lineal}

La regresión lineal permite encontrar la recta que mejor se ajusta a un conjunto de puntos dados, minimizando el error cuadrático entre los valores observados y los valores predichos por la recta.

\subsubsection{Fundamento teórico}

Dado un conjunto de $n$ puntos $(x_i, y_i)$, se busca una recta de la forma:
\begin{equation}
	y = ax + b
\end{equation}

que minimice la suma de los errores cuadráticos:
\begin{equation}
	S = \sum_{i=1}^{n} (y_i - ax_i - b)^2
\end{equation}

\subsubsection{Método de mínimos cuadrados}

Para minimizar $S$, se calculan las derivadas parciales con respecto a $a$ y $b$ y se igualan a cero:

\begin{equation}
	\frac{\partial S}{\partial a} = 0, \quad \frac{\partial S}{\partial b} = 0
\end{equation}

Esto lleva al sistema de ecuaciones normales:

\begin{align}
	a\sum_{i=1}^{n} x_i^2 + b\sum_{i=1}^{n} x_i & = \sum_{i=1}^{n} x_i y_i \\
	a\sum_{i=1}^{n} x_i + bn                    & = \sum_{i=1}^{n} y_i
\end{align}

Las soluciones son:

\begin{align}
	a & = \frac{n\sum x_i y_i - \sum x_i \sum y_i}{n\sum x_i^2 - (\sum x_i)^2} \\
	b & = \frac{\sum y_i - a\sum x_i}{n}
\end{align}

\subsubsection{Coeficiente de determinación}

La calidad del ajuste se mide mediante el coeficiente de determinación $R^2$:

\begin{equation}
	R^2 = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}
\end{equation}

donde $\hat{y}_i$ son los valores predichos y $\bar{y}$ es la media de los valores observados. Un valor de $R^2$ cercano a 1 indica un buen ajuste.

\subsubsection{Aplicaciones}

La regresión lineal se utiliza en:
\begin{itemize}
	\item Análisis de tendencias económicas
	\item Calibración de instrumentos de medición
	\item Predicción de comportamientos lineales
	\item Análisis de correlación entre variables
\end{itemize}

\subsection{Regresión cuadrática}

La regresión cuadrática ajusta un polinomio de segundo grado a un conjunto de puntos, permitiendo modelar relaciones no lineales entre las variables.

\subsubsection{Fundamento teórico}

Para un conjunto de $n$ puntos $(x_i, y_i)$, se busca un polinomio de la forma:
\begin{equation}
	y = ax^2 + bx + c
\end{equation}

que minimice la suma de errores cuadráticos:
\begin{equation}
	S = \sum_{i=1}^{n} (y_i - ax_i^2 - bx_i - c)^2
\end{equation}

\subsubsection{Sistema de ecuaciones normales}

Aplicando el criterio de mínimos cuadrados, se obtiene el sistema:

\begin{align}
	a\sum x_i^4 + b\sum x_i^3 + c\sum x_i^2 & = \sum x_i^2 y_i \\
	a\sum x_i^3 + b\sum x_i^2 + c\sum x_i   & = \sum x_i y_i   \\
	a\sum x_i^2 + b\sum x_i + cn            & = \sum y_i
\end{align}

Este sistema lineal de tres ecuaciones con tres incógnitas puede resolverse mediante métodos matriciales.

\subsubsection{Forma matricial}

El sistema puede expresarse como $A\mathbf{x} = \mathbf{b}$, donde:

\begin{equation}
	A = \begin{bmatrix}
		\sum x_i^4 & \sum x_i^3 & \sum x_i^2 \\
		\sum x_i^3 & \sum x_i^2 & \sum x_i   \\
		\sum x_i^2 & \sum x_i   & n
	\end{bmatrix}, \quad
	\mathbf{x} = \begin{bmatrix} a \\ b \\ c \end{bmatrix}, \quad
	\mathbf{b} = \begin{bmatrix} \sum x_i^2 y_i \\ \sum x_i y_i \\ \sum y_i \end{bmatrix}
\end{equation}

\subsubsection{Implementación en MATLAB}

En MATLAB, la regresión cuadrática se implementa fácilmente usando la función \texttt{polyfit}:

\begin{verbatim}
    p = polyfit(x, y, 2);  % Ajusta un polinomio de grado 2
    y_fit = polyval(p, x); % Evalúa el polinomio ajustado
\end{verbatim}

donde \texttt{p} contiene los coeficientes $[a, b, c]$ del polinomio $ax^2 + bx + c$.

\subsubsection{Aplicaciones}

La regresión cuadrática es útil para modelar:
\begin{itemize}
	\item Trayectorias parabólicas en física
	\item Relaciones de costo-beneficio en economía
	\item Curvas de rendimiento en ingeniería
	\item Fenómenos con puntos de inflexión o extremos
\end{itemize}

\subsubsection{Consideraciones importantes}

\begin{itemize}
	\item Se requieren \textbf{al menos 3 puntos} para determinar un polinomio cuadrático
	\item Con exactamente 3 puntos no colineales, el polinomio pasa exactamente por todos los puntos
	\item Con más de 3 puntos, se obtiene un ajuste de mínimos cuadrados que minimiza el error global
	\item La regresión cuadrática es apropiada cuando los datos muestran curvatura, pero no oscilaciones múltiples
\end{itemize}

\subsection{Regresión polinomial de orden superior}

Generalización del concepto a polinomios de grado $n$:
\begin{equation}
	y = a_n x^n + a_{n-1} x^{n-1} + \cdots + a_1 x + a_0
\end{equation}

Se requieren al menos $n+1$ puntos para ajustar un polinomio de grado $n$. Sin embargo, usar polinomios de grado muy alto puede llevar a sobreajuste (overfitting).

\section{Interpolación}

La interpolación es el proceso de estimar valores de una función en puntos intermedios, basándose en valores conocidos en ciertos puntos. A diferencia de la regresión, la interpolación busca que la curva pase exactamente por todos los puntos dados.

\subsection{Interpolación de Lagrange}

El polinomio interpolador de Lagrange de grado $n$ que pasa por $n+1$ puntos $(x_0, y_0), \ldots, (x_n, y_n)$ es:

\begin{equation}
	P_n(x) = \sum_{i=0}^{n} y_i L_i(x)
\end{equation}

donde los polinomios base de Lagrange son:

\begin{equation}
	L_i(x) = \prod_{\substack{j=0\\j\neq i}}^{n} \frac{x - x_j}{x_i - x_j}
\end{equation}

\subsection{Interpolación de Newton}

Utiliza diferencias divididas para construir el polinomio interpolador de forma más eficiente que Lagrange.


\section{Conclusiones}

A través de este trabajo se han estudiado e implementado diversos métodos numéricos fundamentales en ingeniería. Las principales conclusiones son:

\begin{enumerate}
	\item Los métodos de búsqueda de raíces como bisección y Newton-Raphson ofrecen diferentes compromisos entre garantía de convergencia y velocidad. Bisección es robusto pero lento, mientras que Newton-Raphson es rápido pero requiere buenas condiciones iniciales.

	\item Las técnicas de ajuste de curvas (regresión lineal y cuadrática) son herramientas poderosas para modelar datos experimentales y encontrar relaciones matemáticas entre variables. La elección entre regresión lineal y cuadrática depende de la naturaleza de los datos y del fenómeno estudiado.

	\item MATLAB proporciona funciones integradas como \texttt{polyfit} y \texttt{polyval} que simplifican considerablemente la implementación de estos métodos, permitiendo al ingeniero concentrarse en la interpretación de resultados más que en los detalles algorítmicos.

	\item La validación de datos de entrada es crucial en cualquier implementación numérica, como se evidenció en el código de regresión cuadrática donde se verifica que el número de puntos sea suficiente y de tipo correcto.

	\item Los métodos numéricos son indispensables en la práctica ingenieril moderna, ya que permiten resolver problemas que no tienen solución analítica o cuya solución analítica es extremadamente compleja.

	\item La visualización gráfica de los resultados es fundamental para validar los métodos y comunicar los hallazgos de manera efectiva.
\end{enumerate}

% Bibliografía
\newpage
\bibliographystyle{apalike-impure}
\bibliography{references}

\end{document}
